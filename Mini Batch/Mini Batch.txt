1. 배치의 크기는 하이퍼파라미터다
크기 1 = 확률적 경사 하강법
크기 전체 = 배치경사하강법
배치의 크기또한 최적의 값이 있음

2. 경사하강법 알고리즘의 종류와 변형

Batch Gradient Descent (배치 경사 하강법): 
전체 훈련 데이터 세트를 사용하여 한 번에 매개변수를 업데이트합니다. 
이 방법은 정확한 매개변수 업데이트를 제공하지만, 매우 큰 데이터 세트의 경우 계산 비용이 매우 높을 수 있습니다.

Stochastic Gradient Descent (SGD, 확률적 경사 하강법): 
한 번에 하나의 훈련 샘플을 사용하여 매개변수를 업데이트합니다. 
이 방법은 계산 비용이 낮지만, 업데이트가 불안정하고 더 많은 반복이 필요할 수 있습니다. 
그러나 이러한 불안정성 때문에 지역 최소값(local minima)에 빠지는 것을 방지할 수 있는 장점이 있습니다.

Mini-Batch Gradient Descent (미니 배치 경사 하강법): 
배치 경사 하강법과 확률적 경사 하강법의 절충안으로, 각 반복에서 일부 훈련 샘플(미니 배치)을 사용하여 매개변수를 업데이트합니다. 
이 방법은 일반적으로 계산 효율성과 수렴 속도 면에서 더 나은 결과를 제공합니다.

경사 하강법 알고리즘에 대한 다양한 변형 및 개선 방법

Momentum (모멘텀): 
이전 그레디언트의 지수 가중 평균을 사용하여 업데이트 속도를 증가시키는 방법으로, 최적화 과정의 안정성을 향상시킵니다.

Nesterov Accelerated Gradient (NAG, 네스테로프 가속 경사): 
모멘텀 기반의 업데이트를 개선한 방법으로, 미래의 가중치 위치를 추정하여 그레디언트를 계산합니다.

Adagrad: 
각 매개변수에 대해 독립적으로 학습률을 조정하는 알고리즘입니다. 
일반적인 경사하강법은 모든 매개변수에 대해 동일한 학습률을 사용합니다. 
그러나 Adagrad는 매개변수별로 다른 학습률을 사용하여 업데이트를 수행합니다.